---
layout: page
title: Introduction to Automatic Differentiation Variational Inference
description: Intro to ADVI
lang: "python"
math: on
---

# {{page.title}}

This page was last updated on {{ "now" | date: "%d %b, %Y" }}.

In this post, I will give a brief overview of Automatic Differentiation
Variational Inference (ADVI). Basic knowledge of Bayesian inference is assumed.
I have based most of this post on [this paper][1].

## Bayesian Inference
Practitioners of Bayesian methodology are typically interested in posterior distributions
of model parameters. That is, given a model $p(y \mid \theta)$, where $y$ are data and 
$\theta$ are model parameters, one may be interested in $p(\theta \mid y)$. Using Bayes rule,
we know that

$$
p(\theta \mid y) = \frac{ p(\theta) p(y\mid\theta) }{\int p(\theta) p(y\mid\theta) d\Theta}
$$

where $\Theta$ is the parameter space of $\theta$. Typically, the integral in the
denominator of this expression is intractable (or cannot be obtained in closed
form). One way to obtain the posterior distribution is to sample from the
distribution via Markov Chain Monte Carlo (MCMC). These methods can be
computationally expensive, and inefficient. That is, due to the nature of the
implementation of certain sampling procedures, obtaining $B$ independent
samples may require sampling $B^* > B$ samples, and then taking $B$ sub-samples
to yield samples that are less auto-correlated. This procedure is called
thinning.  MCMC is able to provide excellent solutions to complex models,
though at times can be prohibitively slow.

## Variational Inference (VI)
Variational inference is an alternative to MCMC for fitting Bayesian models. 
Practitioners of this method are interested in the posterior distribution
of model parameters, but are typically seeking methods that are faster and
scalable to datasets that are large (in terms of number of observations).

## ADVI

[1]: https://arxiv.org/abs/1603.00788
