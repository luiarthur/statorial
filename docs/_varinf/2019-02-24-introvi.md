---
layout: page
title: Introduction to Automatic Differentiation Variational Inference
description: Intro to ADVI
lang: "python"
math: on
---

$$
\def\data{\operatorname{data}}
$$

# {{page.title}}

This page was last updated on {{ "now" | date: "%d %b, %Y" }}.

In this post, I will give a brief overview of Automatic Differentiation
Variational Inference (ADVI). Basic knowledge of Bayesian inference is assumed.
Familiarity with gradient-based optimization methods like gradient descent will
be helpful.  I have based most of this post on [this paper][1]. This post is
not meant to be comprehensive, but simply to provide a high-level overview to
those who are interested in implementing ADVI. Those seeking a more in-depth
treatment should review the papers linked in this post.

## Bayesian Inference
Practitioners of Bayesian methodology are typically interested in posterior
distributions of model parameters. That is, given a model $p(\data \mid
\theta)$, where  $\theta$ are model parameters, one may be interested in
$p(\theta \mid \data)$. Using Bayes rule, we know that

$$
p(\theta \mid \data) = \frac{ p(\theta) p(\data\mid\theta) }{\int p(\theta)
p(\data\mid\theta) d\Theta}
$$

where $\Theta$ is the parameter space of $\theta$. Typically, the integral in the
denominator of this expression is intractable (or cannot be obtained in closed
form). One way to obtain the posterior distribution is to sample from the
distribution via Markov Chain Monte Carlo (MCMC). These methods can be
computationally expensive, and inefficient. That is, due to the nature of the
implementation of certain sampling procedures, obtaining $B$ independent
samples may require sampling $B^* > B$ samples, and then taking $B$ sub-samples
to yield samples that are less auto-correlated. This procedure is called
thinning.  MCMC is able to provide excellent solutions to complex models,
though at times can be prohibitively slow.

## Variational Inference (VI)
Variational inference is an alternative to MCMC for fitting Bayesian models. 
Practitioners of this method are interested in the posterior distribution
of model parameters, but are typically seeking methods that are faster and
scalable to datasets that are large (in terms of number of observations).

In VI inference, one seeks the exact (analytic) distribution of a "close"
approximation to the true posterior distribution. Formally, one seeks
the solution to the following expression:

$$
\argmin{\phi \in \Phi} \KL\p{q(\theta; \phi) ~||~ p(\theta \mid \data)}.
$$

In the expression above, $q(\theta; \phi)$ is known as the variational density.
It is a parametric density parameterized by $\phi$, which should be easy to
sample from and evaluate. KL (Kullback-Leibler) divergence is a metric to
measure a "distance" between two densities. Hence we are trying to minimize
the dissimilarity between the approximating class of distributions and the
true posterior. The exact form of KL between two densities is:

$$
\KL\p{q(\theta)} ~~||~~ p(\theta)) = \E_q\bk{\log q(\theta) - \log p(\theta)}
$$

where the expectation is taken with respect $Q$ -- which in VI would be the
variational distribution for $\theta$.

Directly minimizing the KL divergence is difficult. Consequently, practitioners
try to minimize the *evidence lower bound* (ELBO), which is equivalent to 
minimizing the KL divergence. The ELBO is:

$$
\elbo(\phi) = \E_{q(\theta;\phi)}\bk{\log p(\data\mid\theta) + \log p(\theta) - \log q(\theta)}.
$$

Therefore, the objective in VI becomes finding solutions to:

$$
\phi^* = \argmax{\phi} \elbo(\phi).
$$

Here, I will leave the reader to seek on their own how this is classically
done.  [This review][2] outlines the coordinate ascent variational inference
(CAVI) algorithm for finding a solution for $\phi^*$. Suffice it to say that in
CAVI, one tries to update the parameters $\phi$ in the variational distribution
sequentially, until some convergence criteria is met. This requires
analytic-derivations of the updates, which can be time-consuming at best, and
infeasible in some cases. For instance, closed-form CAVI updates are not available
in a logistic regression.

Below are some items to be cautious about:
- Local modes
    - In iterative methods for finding a global maximum, CAVI and other VI
      implementations may go to one of many local modes instead.
- Selecting $q(\theta)$
    - The most important aspect of selecting $q(\theta)$ is ensuring the
      support of $q(\theta)$ matches that of the prior $p(\theta)$.
- Assessing convergence (to a local mode)
    - One can assess convergence by checking if the current evaluation of the ELBO 
      changed from the previous by less than a certain threshold.

## ADVI
As previously mentioned, CAVI is one way to maximize the ELBO. However,
deriving the updates for CAVI can be difficult or impossible even in common
cases. An alternative method to maximize the ELBO is automatic differentiation
variational inference (ADVI). This is a gradient based method. One still uses
iterative optimization procedures to obtain $\phi^*$, but instead of CAVI, one
use something like (stochastic) gradient descent. This requires computing the
derivatives of ELBO with respect to the parameters. This can be very tedious in
complex models. In the presence of automatic differentiation (AD) libraries
(like `autograd` and `PyTorch`in python), one can rely on libraries to compute
derivatives accurately. How these libraries compute derivatives is out of the
scope of this post. Just note that in AD libraries like those in `PyTorch`,
derivatives are not computed numerically (in the traditional sense), nor
symbolically. AD relies a representation of variables known as dual numbers to 
cleverly compute gradients efficiently. It is also necessary to store
computational graphs and perform some sort of source code generation.
[This paper][4] surveys how AD is used in machine learning.

<!-- TODO -->
[MORE COMING SOON ...]

## Other useful resources
- [Stochastic Variational Inference (SVI)][3]

[1]: https://arxiv.org/abs/1603.00788
[2]: https://arxiv.org/abs/1601.00670
[3]: http://www.jmlr.org/papers/v14/hoffman13a.html
[4]: http://www.jmlr.org/papers/volume18/17-468/17-468.pdf
